# SnapRAG çš„ RAG å®ç°è¯¦è§£

## ğŸ“– æ¦‚è¿°

SnapRAGå®ç°äº†å®Œæ•´çš„**RAG (Retrieval-Augmented Generation)** pipelineï¼Œç”¨äºæ™ºèƒ½æŸ¥è¯¢Farcasterç”¨æˆ·æ•°æ®å’ŒCastå†…å®¹ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
ç”¨æˆ·æŸ¥è¯¢
    â†“
ã€1. æ£€ç´¢é˜¶æ®µ Retrievalã€‘
    â”œâ”€ Semantic Search (è¯­ä¹‰æœç´¢)
    â”œâ”€ Keyword Search (å…³é”®è¯æœç´¢)  
    â”œâ”€ Hybrid Search (æ··åˆæœç´¢)
    â””â”€ Auto Search (æ™ºèƒ½é€‰æ‹©)
    â†“
ã€2. æ’åºé˜¶æ®µ Rankingã€‘
    â”œâ”€ Vector Similarity (å‘é‡ç›¸ä¼¼åº¦)
    â”œâ”€ RRF Fusion (å€’æ•°æ’åèåˆ)
    â””â”€ Score Normalization (åˆ†æ•°å½’ä¸€åŒ–)
    â†“
ã€3. ä¸Šä¸‹æ–‡ç»„è£… Context Assemblyã€‘
    â”œâ”€ Profile/Cast æ ¼å¼åŒ–
    â”œâ”€ Author Information (ä½œè€…ä¿¡æ¯)
    â”œâ”€ Engagement Metrics (äº’åŠ¨æŒ‡æ ‡)
    â””â”€ Context Size Management (é•¿åº¦ç®¡ç†)
    â†“
ã€4. ç”Ÿæˆé˜¶æ®µ Generationã€‘
    â”œâ”€ Prompt Template (æç¤ºæ¨¡æ¿)
    â”œâ”€ LLM Query (OpenAI/Ollama)
    â””â”€ Streaming Response (æµå¼å“åº”)
    â†“
æœ€ç»ˆç­”æ¡ˆ + æ¥æº
```

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. Profile RAGï¼ˆç”¨æˆ·æ¡£æ¡ˆæŸ¥è¯¢ï¼‰

**åŠŸèƒ½**ï¼šåŸºäºç”¨æˆ·bioã€å…´è¶£ã€ç¤¾äº¤ä¿¡æ¯çš„æ™ºèƒ½æ£€ç´¢

**æ”¯æŒçš„æ£€ç´¢æ–¹å¼**ï¼š
- âœ… **Semantic Search**: è¯­ä¹‰ç†è§£ï¼ˆ"æ‰¾çƒ­çˆ±AIçš„å¼€å‘è€…"ï¼‰
- âœ… **Keyword Search**: ç²¾ç¡®åŒ¹é…ï¼ˆ"Ethereum" "Solana"ï¼‰
- âœ… **Hybrid Search**: æ··åˆæœç´¢ï¼ˆRRFèåˆï¼‰
- âœ… **Auto Search**: æ™ºèƒ½é€‰æ‹©æœ€ä½³æ–¹æ³•

**æ•°æ®æ¥æº**ï¼š
```sql
user_profiles + profile_embeddings (bio + metadataçš„1536ç»´å‘é‡)
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
snaprag rag query "Find developers building on Farcaster"
```

### 2. Cast RAGï¼ˆå†…å®¹æŸ¥è¯¢ï¼‰

**åŠŸèƒ½**ï¼šåŸºäºCastæ–‡æœ¬å†…å®¹çš„æ™ºèƒ½æ£€ç´¢ï¼ŒåŒ…å«äº’åŠ¨æ•°æ®

**æ”¯æŒçš„æ£€ç´¢æ–¹å¼**ï¼š
- âœ… **Semantic Search**: æ¦‚å¿µåŒ¹é…ï¼ˆ"å…³äºframesçš„è®¨è®º"ï¼‰
- âœ… **Keyword Search**: å…³é”®è¯åŒ¹é…
- âœ… **Hybrid Search**: RRFèåˆ
- âœ… **Thread Retrieval**: å®Œæ•´å¯¹è¯çº¿ç¨‹
- âœ… **FID Filtered**: æŒ‰ç”¨æˆ·ç­›é€‰
- âœ… **Time Range**: æ—¶é—´èŒƒå›´è¿‡æ»¤

**å¢å¼ºæ•°æ®**ï¼š
```rust
CastSearchResult {
    text: String,           // Castå†…å®¹
    similarity: f32,        // ç›¸ä¼¼åº¦åˆ†æ•°
    reply_count: i64,       // å›å¤æ•°
    reaction_count: i64,    // ååº”æ•°
    author_info: Profile,   // ä½œè€…ä¿¡æ¯
}
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
snaprag rag query-casts "What are people saying about Farcaster frames?"
```

## ğŸ” æ£€ç´¢æ–¹æ³•è¯¦è§£

### æ–¹æ³•1: Semantic Searchï¼ˆè¯­ä¹‰æœç´¢ï¼‰

**åŸç†**ï¼š
1. ç”¨æˆ·æŸ¥è¯¢ â†’ ç”Ÿæˆembeddingå‘é‡ï¼ˆ1536ç»´ï¼‰
2. åœ¨å‘é‡æ•°æ®åº“ä¸­è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
3. è¿”å›æœ€ç›¸ä¼¼çš„ç»“æœ

**ä¼˜åŠ¿**ï¼š
- âœ… ç†è§£è¯­ä¹‰ï¼š"AIå¼€å‘è€…" â‰ˆ "æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ"
- âœ… è·¨è¯­è¨€åŒ¹é…ï¼ˆembeddingå·²ç¼–ç è¯­ä¹‰ï¼‰
- âœ… å®¹å¿æ‹¼å†™é”™è¯¯

**å®ç°**ï¼š
```rust
pub async fn semantic_search(
    &self,
    query: &str,
    limit: usize,
    threshold: Option<f32>,
) -> Result<Vec<SearchResult>> {
    // 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
    let query_embedding = self.embedding_service.generate(query).await?;
    
    // 2. å‘é‡ç›¸ä¼¼åº¦æœç´¢
    let profiles = self.database
        .semantic_search_profiles(query_embedding, limit, threshold)
        .await?;
    
    // 3. è½¬æ¢ä¸ºSearchResult
    Ok(profiles.into_iter().map(|p| SearchResult { ... }).collect())
}
```

**SQLæŸ¥è¯¢**ï¼š
```sql
SELECT *, 
       1 - (embedding <=> $1) as similarity
FROM profile_embeddings
WHERE 1 - (embedding <=> $1) > $threshold
ORDER BY embedding <=> $1
LIMIT $limit
```

### æ–¹æ³•2: Keyword Searchï¼ˆå…³é”®è¯æœç´¢ï¼‰

**åŸç†**ï¼š
- SQL `ILIKE` æ¨¡å¼åŒ¹é…
- åœ¨bioã€usernameã€display_nameä¸­æœç´¢

**ä¼˜åŠ¿**ï¼š
- âœ… ç²¾ç¡®åŒ¹é…ç‰¹å®šè¯æ±‡
- âœ… é€‚åˆæœç´¢ä¸“æœ‰åè¯ï¼ˆé¡¹ç›®åã€å…¬å¸åï¼‰
- âœ… å¿«é€Ÿã€å¯é¢„æµ‹

**å®ç°**ï¼š
```rust
pub async fn keyword_search(
    &self, 
    query: &str, 
    limit: usize
) -> Result<Vec<SearchResult>> {
    // åœ¨bioã€usernameç­‰å­—æ®µä¸­æœç´¢å…³é”®è¯
    let profiles = self.database
        .search_profiles_by_keyword(query, limit)
        .await?;
    
    Ok(profiles.into_iter()
        .map(|p| SearchResult {
            score: 0.8,  // å›ºå®šåˆ†æ•°
            match_type: MatchType::Keyword,
            ...
        })
        .collect())
}
```

### æ–¹æ³•3: Hybrid Searchï¼ˆæ··åˆæœç´¢ï¼‰

**åŸç†**ï¼šRRF (Reciprocal Rank Fusion) èåˆç®—æ³•

**å…¬å¼**ï¼š
```
RRF_score(doc) = Î£ 1 / (k + rank_i(doc))

å…¶ä¸­ï¼š
- k = 60 (å¸¸æ•°)
- rank_i = æ–‡æ¡£åœ¨ç¬¬iä¸ªæ’åºåˆ—è¡¨ä¸­çš„æ’å
```

**æµç¨‹**ï¼š
```rust
pub async fn hybrid_search(
    &self,
    query: &str,
    limit: usize,
) -> Result<Vec<SearchResult>> {
    // 1. å¹¶è¡Œæ‰§è¡Œä¸¤ç§æœç´¢
    let (semantic_results, keyword_results) = tokio::join!(
        self.semantic_search(query, limit * 2, None),
        self.keyword_search(query, limit * 2)
    );
    
    // 2. RRFèåˆ
    let merged = self.merge_results_rrf(
        semantic_results?, 
        keyword_results?, 
        limit
    );
    
    Ok(merged)
}

fn merge_results_rrf(&self, ...) -> Vec<SearchResult> {
    // RRFç®—æ³•å®ç°
    for (rank, result) in semantic_results.iter().enumerate() {
        scores[fid] += 1.0 / (60.0 + rank as f32);
    }
    for (rank, result) in keyword_results.iter().enumerate() {
        scores[fid] += 1.0 / (60.0 + rank as f32);
    }
    // æŒ‰RRFåˆ†æ•°æ’åºè¿”å›
}
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç»“åˆè¯­ä¹‰ç†è§£å’Œç²¾ç¡®åŒ¹é…
- âœ… æ›´å¥½çš„å¬å›ç‡å’Œå‡†ç¡®ç‡
- âœ… å¹³è¡¡ä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ£

### æ–¹æ³•4: Auto Searchï¼ˆæ™ºèƒ½é€‰æ‹©ï¼‰

**åŸç†**ï¼šæ ¹æ®æŸ¥è¯¢ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•

**å†³ç­–é€»è¾‘**ï¼š
```rust
fn analyze_query(query: &str) -> RetrievalMethod {
    let lower = query.to_lowercase();
    
    // 1. æ£€æŸ¥æ˜¯å¦ä¸ºç²¾ç¡®æœç´¢ï¼ˆå¼•å·ã€FIDã€ç‰¹å®šå…³é”®è¯ï¼‰
    if query.contains('"') || query.starts_with("fid:") {
        return RetrievalMethod::Keyword;
    }
    
    // 2. æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­æŸ¥è¯¢ï¼ˆ1-2ä¸ªè¯ï¼‰
    let words: Vec<&str> = query.split_whitespace().collect();
    if words.len() <= 2 {
        // çŸ­æŸ¥è¯¢ç”¨å…³é”®è¯åŒ¹é…æ›´å¥½
        return RetrievalMethod::Keyword;
    }
    
    // 3. æ£€æŸ¥ç‰¹æ®Šå…³é”®è¯ï¼ˆä¸“æœ‰åè¯ï¼‰
    let proper_nouns = ["ethereum", "bitcoin", "solana", "base", "optimism"];
    if proper_nouns.iter().any(|&noun| lower.contains(noun)) {
        return RetrievalMethod::Hybrid;  // æ··åˆæœç´¢
    }
    
    // 4. é»˜è®¤ï¼šæ¦‚å¿µæ€§æŸ¥è¯¢ç”¨è¯­ä¹‰æœç´¢
    RetrievalMethod::Semantic
}
```

**ç¤ºä¾‹**ï¼š
- `"Vitalik"` â†’ Keywordï¼ˆç²¾ç¡®æœç´¢ï¼‰
- `"developers building on Base"` â†’ Semanticï¼ˆè¯­ä¹‰ç†è§£ï¼‰
- `"Ethereum developers interested in AI"` â†’ Hybridï¼ˆæ··åˆï¼‰

## ğŸ“¦ ä¸Šä¸‹æ–‡ç»„è£…

### Profile Contextï¼ˆç”¨æˆ·æ¡£æ¡ˆä¸Šä¸‹æ–‡ï¼‰

```rust
pub struct ContextAssembler {
    max_context_length: usize,  // é»˜è®¤4096 tokens
}

impl ContextAssembler {
    pub fn assemble(&self, results: &[SearchResult]) -> String {
        let mut context = String::new();
        let mut current_length = 0;
        
        for (idx, result) in results.iter().enumerate() {
            let profile_text = format!(
                "Profile {}:\n\
                 Username: {}\n\
                 Display Name: {}\n\
                 Bio: {}\n\
                 Location: {}\n\
                 Interests: {}\n\
                 Match Score: {:.2}\n\n",
                idx + 1,
                result.profile.username.unwrap_or("N/A"),
                result.profile.display_name.unwrap_or("N/A"),
                result.profile.bio.unwrap_or("N/A"),
                result.profile.location.unwrap_or("N/A"),
                // ... æ›´å¤šå­—æ®µ
                result.score
            );
            
            // æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + profile_text.len() > self.max_context_length {
                break;
            }
            
            context.push_str(&profile_text);
            current_length += profile_text.len();
        }
        
        context
    }
}
```

### Cast Contextï¼ˆå†…å®¹ä¸Šä¸‹æ–‡ï¼‰

```rust
pub struct CastContextAssembler {
    max_context_length: usize,
}

impl CastContextAssembler {
    pub async fn assemble_with_authors(
        &self,
        results: &[CastSearchResult],
        database: Arc<Database>,
    ) -> Result<String> {
        let mut context = String::new();
        
        for (idx, cast) in results.iter().enumerate() {
            // è·å–ä½œè€…ä¿¡æ¯
            let author = database.get_user_profile(cast.fid).await?;
            let author_name = author
                .and_then(|p| p.username.or(p.display_name))
                .unwrap_or_else(|| format!("FID {}", cast.fid));
            
            let cast_text = format!(
                "Cast {}:\n\
                 Author: {} (FID: {})\n\
                 Content: {}\n\
                 Engagement: {} replies, {} reactions\n\
                 Similarity: {:.1}%\n\n",
                idx + 1,
                author_name,
                cast.fid,
                cast.text,
                cast.reply_count,
                cast.reaction_count,
                cast.similarity * 100.0
            );
            
            if context.len() + cast_text.len() > self.max_context_length {
                break;
            }
            
            context.push_str(&cast_text);
        }
        
        Ok(context)
    }
}
```

## ğŸ¤– LLMç”Ÿæˆ

### Prompt Templatesï¼ˆæç¤ºæ¨¡æ¿ï¼‰

**Profile RAG Prompt**ï¼š
```rust
pub fn build_profile_rag_prompt(query: &str, context: &str) -> String {
    format!(
        "You are a Farcaster data assistant. Based on the following user profiles, \
         answer the question accurately and concisely.\n\n\
         USER PROFILES:\n{}\n\n\
         QUESTION: {}\n\n\
         INSTRUCTIONS:\n\
         - Only use information from the profiles above\n\
         - If the answer cannot be determined, say so\n\
         - Include relevant usernames/FIDs in your answer\n\
         - Be specific and cite sources\n\n\
         ANSWER:",
        context, query
    )
}
```

**Cast RAG Prompt**ï¼š
```rust
pub fn build_cast_rag_prompt(query: &str, context: &str) -> String {
    format!(
        "You are analyzing Farcaster casts. Based on the following casts, \
         answer the question and provide insights.\n\n\
         CASTS:\n{}\n\n\
         QUESTION: {}\n\n\
         INSTRUCTIONS:\n\
         - Summarize key themes and opinions\n\
         - Consider engagement metrics (replies, reactions)\n\
         - Mention notable authors if relevant\n\
         - Be objective and balanced\n\n\
         ANSWER:",
        context, query
    )
}
```

### LLMè°ƒç”¨

```rust
pub struct LlmService {
    client: LlmClient,
}

impl LlmService {
    pub async fn query(
        &self,
        prompt: &str,
        temperature: f32,
        max_tokens: usize,
    ) -> Result<String> {
        match &self.client {
            LlmClient::OpenAI(client) => {
                client.generate(prompt, temperature, max_tokens).await
            }
            LlmClient::Ollama(client) => {
                client.generate(prompt, temperature, max_tokens).await
            }
            LlmClient::Custom(_) => {
                Err(SnapRagError::LlmError(
                    "Custom provider not yet implemented".to_string()
                ))
            }
        }
    }
}
```

## ğŸ¨ ä½¿ç”¨ç¤ºä¾‹

### 1. ProfileæŸ¥è¯¢

```bash
# åŸºç¡€æŸ¥è¯¢
snaprag rag query "Find developers interested in crypto"

# é«˜çº§æŸ¥è¯¢
snaprag rag query "Who are the most active Farcaster builders?" \
  --limit 20 \
  --temperature 0.7 \
  --max-tokens 500
```

**å·¥ä½œæµç¨‹**ï¼š
1. ç”ŸæˆæŸ¥è¯¢embedding
2. è¯­ä¹‰æœç´¢ç”¨æˆ·profiles
3. ç»„è£…top 20ç”¨æˆ·ä¿¡æ¯
4. LLMç”Ÿæˆæ€»ç»“ç­”æ¡ˆ

### 2. CastæŸ¥è¯¢

```bash
# åŸºç¡€æŸ¥è¯¢
snaprag rag query-casts "What are people saying about frames?"

# è¯¦ç»†æŸ¥è¯¢
snaprag rag query-casts "Discussions about Warpcast vs other clients" \
  --limit 15 \
  --threshold 0.7 \
  --verbose
```

**å·¥ä½œæµç¨‹**ï¼š
1. ç”ŸæˆæŸ¥è¯¢embedding
2. è¯­ä¹‰æœç´¢casts
3. è·å–ä½œè€…ä¿¡æ¯å’Œengagement metrics
4. ç»„è£…castå†…å®¹+ä¸Šä¸‹æ–‡
5. LLMåˆ†æç”Ÿæˆæ´å¯Ÿ

### 3. ThreadæŸ¥è¯¢

```bash
# è·å–å®Œæ•´å¯¹è¯
snaprag cast thread <CAST_HASH> --depth 10
```

**è¿”å›ç»“æ„**ï¼š
```
â¬†ï¸ Parent Chain (çˆ¶çº§é“¾)
   â””â”€ Original Cast
       â”œâ”€ Reply 1
       â”‚  â””â”€ Reply 1.1
       â”œâ”€ Reply 2
       â””â”€ Reply 3
```

## ğŸ“Š æ€§èƒ½ç‰¹æ€§

### 1. å‘é‡æœç´¢æ€§èƒ½

```sql
-- IVFFlatç´¢å¼•
CREATE INDEX idx_profile_embeddings 
ON profile_embeddings 
USING ivfflat (embedding vector_l2_ops) 
WITH (lists = 100);

-- Cast embeddingsç´¢å¼•
CREATE INDEX idx_cast_embeddings_embedding 
ON cast_embeddings 
USING ivfflat (embedding vector_l2_ops) 
WITH (lists = 100);
```

**æ€§èƒ½æŒ‡æ ‡**ï¼š
- Profileæœç´¢: ~10ms (10K profiles)
- Castæœç´¢: ~50ms (100K casts)
- Embeddingç”Ÿæˆ: ~200ms (OpenAI)

### 2. ç¼“å­˜ç­–ç•¥

- âœ… Embeddingç¼“å­˜ï¼ˆPostgreSQLå­˜å‚¨ï¼‰
- âœ… Profileç¼“å­˜ï¼ˆåº”ç”¨å±‚ï¼‰
- âœ… æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆå¯é€‰ï¼Œæœªå®ç°ï¼‰

### 3. æ‰¹å¤„ç†

```rust
// Embeddingæ‰¹é‡ç”Ÿæˆ
pub async fn backfill_cast_embeddings(...) {
    const BATCH_SIZE: usize = 100;
    const PARALLEL_TASKS: usize = 5;
    
    stream::iter(casts)
        .map(|cast| process_cast_with_retry(cast, ...))
        .buffered(PARALLEL_TASKS)  // 5å¹¶å‘
        .collect()
        .await
}
```

## ğŸ”§ é…ç½®é€‰é¡¹

### config.toml

```toml
[embeddings]
provider = "openai"  # or "ollama"
model = "text-embedding-3-small"
api_key = "${OPENAI_API_KEY}"

[llm]
provider = "openai"  # or "ollama"
model = "gpt-4"
api_key = "${OPENAI_API_KEY}"
temperature = 0.7
max_tokens = 2000

[rag]
retrieval_limit = 10
context_max_length = 4096
enable_hybrid_search = true
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ£€ç´¢æ–¹æ³•

| æŸ¥è¯¢ç±»å‹ | æ¨èæ–¹æ³• | åŸå›  |
|---------|---------|------|
| æ¦‚å¿µæ€§æŸ¥è¯¢ | Semantic | ç†è§£è¯­ä¹‰ |
| ä¸“æœ‰åè¯ | Keyword | ç²¾ç¡®åŒ¹é… |
| å¤æ‚æŸ¥è¯¢ | Hybrid | ç»¼åˆä¼˜åŠ¿ |
| ä¸ç¡®å®š | Auto | æ™ºèƒ½é€‰æ‹© |

### 2. Contexté•¿åº¦ç®¡ç†

```rust
// æ ¹æ®LLMçš„context windowè°ƒæ•´
let context_assembler = ContextAssembler::new(
    match llm_model {
        "gpt-4" => 8192,      // GPT-4
        "gpt-3.5" => 4096,    // GPT-3.5
        "claude-3" => 100000, // Claude 3
        _ => 4096,            // é»˜è®¤
    }
);
```

### 3. Temperatureè®¾ç½®

```rust
temperature:
    0.0-0.3  â†’ äº‹å®æ€§æŸ¥è¯¢ï¼ˆ"åˆ—å‡ºæ‰€æœ‰..."ï¼‰
    0.4-0.7  â†’ å¹³è¡¡ï¼ˆæ¨èï¼‰
    0.8-1.0  â†’ åˆ›æ„æ€§å›ç­”ï¼ˆ"æƒ³è±¡ä¸€ä¸ª..."ï¼‰
```

## ğŸš€ æœªæ¥å¢å¼º

### P1ä¼˜å…ˆçº§
- [ ] **Query expansion**: æŸ¥è¯¢æ‰©å±•ï¼ˆåŒä¹‰è¯ã€ç›¸å…³è¯ï¼‰
- [ ] **Cross-encoder reranking**: æ›´ç²¾ç¡®çš„é‡æ’åº
- [ ] **Caching layer**: Redisç¼“å­˜çƒ­é—¨æŸ¥è¯¢
- [ ] **Streaming responses**: å®æ—¶æµå¼è¾“å‡º

### P2ä¼˜å…ˆçº§
- [ ] **Multi-hop reasoning**: å¤šè·³æ¨ç†
- [ ] **Query understanding**: æŸ¥è¯¢æ„å›¾åˆ†ç±»
- [ ] **Result explanation**: è§£é‡Šä¸ºä»€ä¹ˆæ£€ç´¢åˆ°æŸä¸ªç»“æœ
- [ ] **Personalization**: åŸºäºç”¨æˆ·å†å²çš„ä¸ªæ€§åŒ–

### P3ä¼˜å…ˆçº§
- [ ] **Knowledge graphs**: ç”¨æˆ·å…³ç³»å›¾è°±
- [ ] **Temporal awareness**: æ—¶é—´æ„ŸçŸ¥ï¼ˆ"æœ€è¿‘çš„è¶‹åŠ¿"ï¼‰
- [ ] **Multi-modal**: æ”¯æŒå›¾ç‰‡ã€è§†é¢‘
- [ ] **Evaluation metrics**: RAGè´¨é‡è¯„ä¼°

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `RAG_USAGE.md` - ä½¿ç”¨æŒ‡å—
- `IMPLEMENTATION_SUMMARY.md` - å®ç°æ€»ç»“
- `src/rag/` - æºä»£ç 
- `src/tests/rag_integration_test.rs` - é›†æˆæµ‹è¯•

---

**SnapRAGçš„RAGç³»ç»Ÿæ˜¯ä¸€ä¸ªå®Œæ•´ã€é«˜æ€§èƒ½ã€production-readyçš„å®ç°ï¼** ğŸ‰

