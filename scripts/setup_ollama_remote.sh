#!/bin/bash

echo "🔧 Setup Ollama for Remote Access"
echo "=================================="
echo ""
echo "Current Ollama is listening on: 127.0.0.1:11434 (localhost only)"
echo "We need to make it accessible from: 192.168.1.192:11434"
echo ""

echo "🎯 Solution 1: Configure Ollama to Listen on All Interfaces"
echo "------------------------------------------------------------"
echo ""
echo "SSH into 192.168.1.192 and run:"
echo ""
echo "  # Stop Ollama"
echo "  sudo systemctl stop ollama"
echo ""
echo "  # Edit systemd service file"
echo "  sudo nano /etc/systemd/system/ollama.service"
echo ""
echo "  # Add this line under [Service]:"
echo "  Environment=\"OLLAMA_HOST=0.0.0.0:11434\""
echo ""
echo "  # Reload and restart"
echo "  sudo systemctl daemon-reload"
echo "  sudo systemctl start ollama"
echo ""
echo "  # Verify it's listening on all interfaces"
echo "  netstat -an | grep 11434"
echo "  # Should show: 0.0.0.0:11434 instead of 127.0.0.1:11434"
echo ""
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo ""
echo "🎯 Solution 2: Use SSH Tunnel (Quick Test)"
echo "--------------------------------------------"
echo ""
echo "Run this in a separate terminal:"
echo "  ssh -L 11434:localhost:11434 ryan@192.168.1.192 -N"
echo ""
echo "Then update config.toml:"
echo "  llm_endpoint = \"http://localhost:11434\""
echo ""
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo ""
echo "🎯 Solution 3: Remote Execution (Alternative)"
echo "------------------------------------------------"
echo ""
echo "Execute embeddings remotely via SSH and sync results back"
echo "(More complex, not recommended for initial setup)"
echo ""

